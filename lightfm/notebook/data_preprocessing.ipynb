{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8086de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # 원본 CSV 파일\n",
    "    books = pd.read_csv('../data/Books.csv', low_memory=False)\n",
    "    ratings = pd.read_csv('../data/Ratings.csv', low_memory=False)\n",
    "    users = pd.read_csv('../data/Users.csv', low_memory=False)\n",
    "    print(\"Complete download 3 CSV file\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error : ../data/ 폴더에 CSV 파일이 있는지 확인하세요.\")\n",
    "    books, ratings, users = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d867626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신규 고객 유저 데이터\n",
    "new_users_data = {\n",
    "    'User-ID': [300000, 300001, 300002],\n",
    "    'Location': ['usa', 'georgia, usa', 'morrow, georgia, usa'],\n",
    "    'Age': [30, 30, 30] \n",
    "}\n",
    "\n",
    "new_users_df = pd.DataFrame(new_users_data)\n",
    "\n",
    "# 기존 users DataFrame에 신규 고객 데이터를 추가\n",
    "users = pd.concat([users, new_users_df], ignore_index=True) \n",
    "\n",
    "print(f\"신규 고객 3명 삽입 완료. 총 유저 수: {len(users)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c669cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 병합\n",
    "\n",
    "if not (ratings.empty or books.empty or users.empty):\n",
    "    # 'Ratings'를 중심으로 'Books' 정보와 'Users' 정보를 병합\n",
    "    df_merged = pd.merge(ratings, books, on='ISBN')\n",
    "    df_final = pd.merge(df_merged, users, on='User-ID')\n",
    "    print(\"Complete data merge\")\n",
    "else:\n",
    "    print(\"Error: 데이터 병합 실패.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN 및 Outlier 처리\n",
    "import numpy as np\n",
    "\n",
    "# Age (나이) 처리\n",
    "# 비정상적인 값(5세 미만, 100세 초과)과 NaN을 중앙값으로 대체\n",
    "# 유효한 나이 범위(5~100) 내의 데이터 중앙값을 계산\n",
    "median_age = df_final[(df_final['Age'] > 5) & (df_final['Age'] < 100)]['Age'].median()\n",
    "\n",
    "# 비정상적인 나이 값(5 미만, 100 초과)을 중앙값으로 대체\n",
    "df_final['Age'] = np.where(df_final['Age'] > 100, median_age, df_final['Age'])\n",
    "df_final['Age'] = np.where(df_final['Age'] < 5, median_age, df_final['Age'])\n",
    "\n",
    "# NaN 값 중앙값으로 채우기\n",
    "df_final['Age'].fillna(median_age, inplace=True)\n",
    "df_final['Age'] = df_final['Age'].astype(int)\n",
    "\n",
    "# Year-Of-Publication (출판 연도) 처리\n",
    "# 0이거나 현재 연도(2025년 기준)를 초과하는 비정상적인 값을 처리\n",
    "current_year = 2025 \n",
    "\n",
    "# 데이터 로드 시 str로 인식된 Year-Of-Publication 다시 int로 변환\n",
    "df_final['Year-Of-Publication'] = pd.to_numeric(\n",
    "    df_final['Year-Of-Publication'], \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 유효한 연도 범위(1900년 이후 ~ 현재) 내의 데이터 중앙값을 계산\n",
    "median_year = df_final[(df_final['Year-Of-Publication'] > 1900) & (df_final['Year-Of-Publication'] <= current_year)]['Year-Of-Publication'].median()\n",
    "\n",
    "# 0과 현재 연도를 초과하는 연도를 중앙값으로 대체\n",
    "df_final['Year-Of-Publication'] = np.where(df_final['Year-Of-Publication'] > current_year, median_year, df_final['Year-Of-Publication'])\n",
    "df_final['Year-Of-Publication'] = np.where(df_final['Year-Of-Publication'] == 0, median_year, df_final['Year-Of-Publication'])\n",
    "\n",
    "# 처리되지 않은 NaN 값을 중앙값으로 대체\n",
    "df_final['Year-Of-Publication'].fillna(median_year, inplace=True)\n",
    "\n",
    "df_final['Year-Of-Publication'] = df_final['Year-Of-Publication'].astype(int)\n",
    "\n",
    "print(\"Age 및 Year-Of-Publication 결측치/이상치 처리 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd53d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 노이즈 유저 및 아이템 제거 (데이터 품질 향상)\n",
    "\n",
    "## 최소 상호작용 횟수\n",
    "#MIN_USER_INTERACTIONS = 3\n",
    "#MIN_ITEM_INTERACTIONS = 1\n",
    "\n",
    "#print(f\"--- 데이터 필터링 시작 (User 최소 {MIN_USER_INTERACTIONS}회, Item 최소 {MIN_ITEM_INTERACTIONS}회) ---\")\n",
    "#initial_rows = len(df_final)\n",
    "\n",
    "## 노이즈 유저 제거 (상호작용 3회 미만)\n",
    "#user_counts = df_final['User-ID'].value_counts()\n",
    "#valid_users = user_counts[user_counts >= MIN_USER_INTERACTIONS].index\n",
    "#df_final = df_final[df_final['User-ID'].isin(valid_users)]\n",
    "#print(f\"-> 유저 제거 후 상호작용: {len(df_final)}개\")\n",
    "\n",
    "## 노이즈 아이템 제거 (상호작용 1회 미만)\n",
    "#item_counts = df_final['ISBN'].value_counts()\n",
    "#valid_items = item_counts[item_counts >= MIN_ITEM_INTERACTIONS].index\n",
    "#df_final = df_final[df_final['ISBN'].isin(valid_items)]\n",
    "#print(f\"-> 아이템 제거 후 최종 상호작용: {len(df_final)}개\")\n",
    "\n",
    "#final_rows = len(df_final)\n",
    "#print(f\"--- 필터링 완료: {initial_rows}개에서 {final_rows}개로 감소 ({100*(1 - final_rows/initial_rows):.2f}% 감소) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from lightfm.data import Dataset\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# PorterStemmer 초기화\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# text 데이터에 Stemming과 토큰화를 적용하는 함수\n",
    "def stemming_tokenizer(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # 특수문자 및 숫자를 제거 (NLP 정제)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) \n",
    "    # 소문자 변환 후 토큰화 및 Stemming 적용\n",
    "    return [stemmer.stem(w) for w in text.lower().split() if w]\n",
    "\n",
    "# Location feature 정제 및 결합\n",
    "def safe_split(location_str):\n",
    "    if not isinstance(location_str, str): \n",
    "        return pd.Series(['', ''])\n",
    "    parts = location_str.split(', ')\n",
    "    # 국가와 시/도/군을 분리\n",
    "    country = parts[-1] if len(parts) > 0 and parts[-1] else ''\n",
    "    city_state = parts[0] if len(parts) > 0 and parts[0] else ''\n",
    "    return pd.Series([city_state, country])\n",
    "\n",
    "# 데이터 정제 및 특징 결합\n",
    "df_final['Location'] = df_final['Location'].fillna('')\n",
    "df_final[['Region', 'Country']] = df_final['Location'].apply(safe_split)\n",
    "df_final['location_content'] = (df_final['Region'].fillna('') + ' ' + df_final['Country'].fillna('')).astype(str)\n",
    "\n",
    "# 책 내용 특징 결합\n",
    "fill_cols = ['Book-Title', 'Book-Author', 'Publisher']\n",
    "df_final[fill_cols] = df_final[fill_cols].fillna('')\n",
    "df_final['content'] = df_final['Book-Title'] + ' ' + df_final['Book-Author'] + ' ' + df_final['Publisher']\n",
    "\n",
    "print(\"Complete preprocessing steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ID 매핑을 위한 인코더 생성\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "# 신규 고객 ID를 포함\n",
    "user_encoder.fit(users['User-ID'].astype(str))\n",
    "\n",
    "df_final['user_id_mapped'] = user_encoder.transform(df_final['User-ID'].astype(str))\n",
    "df_final['item_id_mapped'] = item_encoder.fit_transform(df_final['ISBN'].astype(str))\n",
    "\n",
    "\n",
    "\n",
    "# 임시 CountVectorizer 객체를 생성하여 feature 이름 목록을 가져옴\n",
    "temp_location_vec = CountVectorizer(tokenizer=stemming_tokenizer)\n",
    "temp_location_vec.fit(df_final['location_content'])\n",
    "location_feature_names = list(temp_location_vec.get_feature_names_out())\n",
    "\n",
    "temp_content_vec = CountVectorizer(tokenizer=stemming_tokenizer)\n",
    "temp_content_vec.fit(df_final['content'])\n",
    "content_feature_names = list(temp_content_vec.get_feature_names_out())\n",
    "\n",
    "# Dataset Fit (정제된 feature 이름 등록)\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users=df_final['user_id_mapped'],\n",
    "    items=df_final['item_id_mapped'],\n",
    "    # Age (Label Binarization)와 Location (CountVectorizer) feature 이름 등록\n",
    "    user_features=['Age'] + location_feature_names, \n",
    "    # Item Content (CountVectorizer)와 Year-Of-Publication (Label Binarization) feature 이름 등록\n",
    "    item_features=content_feature_names + ['Year-Of-Publication']\n",
    ")\n",
    "print(\"Complete LightFM Dataset Fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상호작용(Interactions) 매트릭스 생성\n",
    "\n",
    "# (유저ID, 아이템ID, 평점) 튜플 리스트를 생성\n",
    "interactions_data = list(zip(\n",
    "    df_final['user_id_mapped'], \n",
    "    df_final['item_id_mapped'], \n",
    "    df_final['Book-Rating'].values\n",
    "))\n",
    "\n",
    "# dataset을 사용해 '상호작용' 행렬 생성\n",
    "# interactions: 유저x아이템 행렬 (값이 1이면 상호작용, 0이면 X)\n",
    "# weights: 유저x아이템 행렬 (값이 0~10점인 '평점' 행렬)\n",
    "(interactions, weights) = dataset.build_interactions(interactions_data)\n",
    "\n",
    "print(\"Complete Interactions Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightFM용 피처 매트릭스 생성 (User / Item 분리)\n",
    "\n",
    "# 중복 제거 및 인덱스 정렬\n",
    "df_users = df_final.drop_duplicates(subset='user_id_mapped').set_index('user_id_mapped')\n",
    "df_users = df_users.reindex(range(len(user_encoder.classes_))) \n",
    "df_items = df_final.drop_duplicates(subset='item_id_mapped').set_index('item_id_mapped')\n",
    "df_items = df_items.reindex(range(len(item_encoder.classes_))) \n",
    "\n",
    "# User Features (Age, Location)\n",
    "\n",
    "# Age: Label Binarizer (one-hot)\n",
    "age_binarizer = LabelBinarizer(sparse_output=True)\n",
    "user_age_features = age_binarizer.fit_transform(df_users['Age'].astype(str).values) \n",
    "\n",
    "# Location: Stemming + CountVectorizer\n",
    "location_count_vec = CountVectorizer(tokenizer=stemming_tokenizer)\n",
    "location_count_vec.fit(df_final['location_content']) \n",
    "location_count_matrix = location_count_vec.transform(df_users['location_content'].fillna(''))\n",
    "\n",
    "# Age와 Location 피처를 hstack으로 결합\n",
    "user_features = hstack([user_age_features, location_count_matrix]).tocsr()\n",
    "print(f\"Complete create User feature: {user_features.shape}\") \n",
    "\n",
    "# Item Features (Content, Year)\n",
    "\n",
    "# Book Content (Title, Author, Publisher): Stemming + CountVectorizer\n",
    "content_count_vec = CountVectorizer(tokenizer=stemming_tokenizer)\n",
    "content_count_matrix = content_count_vec.fit_transform(df_items['content'])\n",
    "\n",
    "# Year-Of-Publication: Label Binarizer (one-hot)\n",
    "year_binarizer = LabelBinarizer(sparse_output=True)\n",
    "item_year_features = year_binarizer.fit_transform(df_items['Year-Of-Publication'].astype(str).values)\n",
    "\n",
    "# Item Features 결합\n",
    "item_features = hstack([content_count_matrix, item_year_features]).tocsr()\n",
    "print(f\"Complete create Item feature: {item_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 저장\n",
    "from scipy.sparse import hstack, csr_matrix, save_npz\n",
    "\n",
    "save_npz('../data/interactions.npz', interactions)\n",
    "save_npz('../data/weights.npz', weights)\n",
    "# Age와 Location이 결합된 전체 user_features를 저장\n",
    "save_npz('../data/user_features.npz', user_features) \n",
    "save_npz('../data/item_features.npz', item_features)\n",
    "\n",
    "# 나중에 예측할 때 ID를 변환하기 위해 인코더(Encoder)도 저장\n",
    "with open('../data/encoders.pkl', 'wb') as f:\n",
    "    pickle.dump({'user_encoder': user_encoder, 'item_encoder': item_encoder}, f)\n",
    "\n",
    "print(\"../data/interactions.npz\")\n",
    "print(\"../data/weights.npz\")\n",
    "print(\"../data/user_features.npz\")\n",
    "print(\"../data/item_features.npz\")\n",
    "print(\"../data/encoders.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
